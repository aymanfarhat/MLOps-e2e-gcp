{
  "components": {
    "comp-clean-data": {
      "executorLabel": "exec-clean-data",
      "inputDefinitions": {
        "parameters": {
          "bq_dataset_id": {
            "parameterType": "STRING"
          },
          "location": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "source_table_path": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "Output": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-prepare-dataset": {
      "executorLabel": "exec-prepare-dataset",
      "inputDefinitions": {
        "parameters": {
          "job_id": {
            "parameterType": "STRING"
          },
          "location": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "Output": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-prepare-features": {
      "executorLabel": "exec-prepare-features",
      "inputDefinitions": {
        "parameters": {
          "bq_dataset_id": {
            "parameterType": "STRING"
          },
          "location": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "source_table_path": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "Output": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-train-save-model": {
      "executorLabel": "exec-train-save-model",
      "inputDefinitions": {
        "parameters": {
          "location": {
            "parameterType": "STRING"
          },
          "model_name": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "source_table_path": {
            "parameterType": "STRING"
          },
          "staging_bucket": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "Output": {
            "parameterType": "STRING"
          }
        }
      }
    }
  },
  "defaultPipelineRoot": "gs://vertexai-demo-pipeline/pipeline_root",
  "deploymentSpec": {
    "executors": {
      "exec-clean-data": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "clean_data"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'bigframes' 'google-cloud-bigquery' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef clean_data(\n    bq_dataset_id: str, project_id: str, location: str, source_table_path: str\n) -> str:\n    \"\"\"\n    Loads data from source BQ table and applies basic cleaning operations. Writes to a new BQ table.\n    \"\"\"\n    import bigframes.pandas as bpd\n\n    target_table = f\"{bq_dataset_id}.chicago_taxi_trips_cleaned\"\n\n    bpd.options.bigquery.project = project_id\n    bpd.options.bigquery.location = location\n    df = bpd.read_gbq(\"SELECT * FROM \" + source_table_path)\n\n    # Remove rows with missing values, duplicates, and invalid values\n    df = df.dropna()\n    df = df.drop_duplicates()\n    df = df[df[\"trip_seconds\"] > 0]\n    df = df[df[\"trip_miles\"] > 0]\n\n    # data after 2018 only\n    df = df[df[\"trip_start_timestamp\"] > \"2018-01-01\"]\n    # Limit to 1000 rows for testing\n    df = df.head(10000)\n\n    df.to_gbq(target_table, if_exists=\"replace\")\n\n    return target_table\n\n"
          ],
          "image": "europe-docker.pkg.dev/vertex-ai/training/tf-cpu.2-14.py310:latest"
        }
      },
      "exec-prepare-dataset": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "prepare_dataset"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-bigquery' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef prepare_dataset(job_id: str, project_id: str, location: str) -> str:\n    \"\"\"\n    Creates the dataset in BigQuery for this pipeline run.\n    \"\"\"\n    from google.cloud import bigquery\n\n    client = bigquery.Client()\n    dataset_id = f\"{project_id}.mlops_train_{job_id}\"\n\n    dataset = bigquery.Dataset(dataset_id)\n    dataset.location = location\n    dataset = client.create_dataset(dataset, exists_ok=True)\n\n    return dataset_id\n\n"
          ],
          "image": "europe-docker.pkg.dev/vertex-ai/training/tf-cpu.2-14.py310:latest"
        }
      },
      "exec-prepare-features": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "prepare_features"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'bigframes' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef prepare_features(\n    bq_dataset_id: str, project_id: str, location: str, source_table_path: str\n) -> str:\n    \"\"\"\n    Loads cleaned data from BQ table and prepares features for training.\n    \"\"\"\n    import bigframes.pandas as bpd\n\n    target_table = f\"{bq_dataset_id}.chicago_taxi_trips_features\"\n\n    bpd.options.bigquery.project = project_id\n    bpd.options.bigquery.location = location\n    df = bpd.read_gbq(source_table_path)\n\n    # Feature engineering\n    df[\"trip_start_timestamp\"] = bpd.to_datetime(df[\"trip_start_timestamp\"])\n    df[\"trip_start_hour\"] = df[\"trip_start_timestamp\"].dt.hour\n    df[\"trip_start_day\"] = df[\"trip_start_timestamp\"].dt.day\n    df[\"trip_start_month\"] = df[\"trip_start_timestamp\"].dt.month\n    df[\"trip_start_year\"] = df[\"trip_start_timestamp\"].dt.year\n\n    @bpd.remote_function(\n        bpd.Series,\n        float,\n        reuse=False,\n        packages=[\"geopy\"],\n        bigquery_connection=\"bigframes-connect\",\n    )\n    def calculate_distance(params) -> float:\n        \"\"\"Returns the distance between two points\"\"\"\n        from geopy.distance import great_circle as GRC\n\n        l1 = params[\"pickup_latitude\"]\n        l2 = params[\"pickup_longitude\"]\n        l3 = params[\"dropoff_latitude\"]\n        l4 = params[\"dropoff_longitude\"]\n\n        p1 = (l1, l2)\n        p2 = (l3, l4)\n\n        return GRC(p1, p2).km\n\n\n    # Drop columns not needed for training\n    df = df.drop(\n        columns=[\n            \"trip_start_timestamp\",\n            \"trip_end_timestamp\",\n            \"pickup_census_tract\",\n            \"dropoff_census_tract\",\n            \"pickup_community_area\",\n            \"dropoff_community_area\",\n            \"fare\",\n            \"tips\",\n            \"tolls\",\n            \"extras\",\n            \"payment_type\",\n            \"company\",\n            \"taxi_id\",\n            \"pickup_location\",\n            \"dropoff_location\",\n        ]\n    )\n\n    # Calculate distance between pickup and dropoff locations\n    columns = [\n        \"pickup_latitude\",\n        \"pickup_longitude\",\n        \"dropoff_latitude\",\n        \"dropoff_longitude\",\n    ]\n\n    df[\"distance\"] = 0\n    df[\"distance\"] = df[columns].apply(calculate_distance, axis=1)\n\n    df.to_gbq(target_table, if_exists=\"replace\")\n\n    return target_table\n\n"
          ],
          "image": "europe-docker.pkg.dev/vertex-ai/training/tf-cpu.2-14.py310:latest"
        }
      },
      "exec-train-save-model": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "train_save_model"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'scikit-learn' 'pandas' 'google-cloud-bigquery' 'bigframes' 'pandas-gbq' 'google-cloud-aiplatform' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef train_save_model(project_id: str, location: str, source_table_path: str, model_name: str, staging_bucket: str) -> str:\n    \"\"\"\n    Trains a model using the prepared features. Correlation is between distance and fare. Using scikit learn regression model.\n    \"\"\"\n    import pandas\n    import bigframes.pandas as bpd\n    import pickle\n    from sklearn import linear_model\n    from google.cloud import aiplatform\n\n    bpd.options.bigquery.project = project_id\n    bpd.options.bigquery.location = location\n    df_bpd = bpd.read_gbq(\n        f\"SELECT trip_seconds, distance, trip_total FROM {source_table_path}\"\n    )\n\n    df = df_bpd.to_pandas()\n\n    # Load data\n    model = linear_model.LinearRegression()\n\n    # Train model\n    model.fit(df[[\"distance\"]], df[\"trip_total\"])\n\n    model_path = \"model.pkl\"\n\n    # Save model locally\n    with open(model_path, \"wb\") as f:\n        pickle.dump(model, f)\n\n    # Upload model to Vertex AI registry\n    model = aiplatform.Model.upload_scikit_learn_model_file(\n        model_file_path=model_path,\n        display_name=model_name,\n        project=project_id,\n        location=location,\n        staging_bucket=staging_bucket,\n    )\n\n    return model_path\n\n"
          ],
          "image": "europe-docker.pkg.dev/vertex-ai/training/tf-cpu.2-14.py310:latest"
        }
      }
    }
  },
  "pipelineInfo": {
    "description": "This is a basic pipeline",
    "name": "my-basic-pipeline"
  },
  "root": {
    "dag": {
      "tasks": {
        "clean-data": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-clean-data"
          },
          "dependentTasks": [
            "prepare-dataset"
          ],
          "inputs": {
            "parameters": {
              "bq_dataset_id": {
                "taskOutputParameter": {
                  "outputParameterKey": "Output",
                  "producerTask": "prepare-dataset"
                }
              },
              "location": {
                "componentInputParameter": "location"
              },
              "project_id": {
                "componentInputParameter": "project_id"
              },
              "source_table_path": {
                "componentInputParameter": "source_table_path"
              }
            }
          },
          "taskInfo": {
            "name": "clean-data"
          }
        },
        "prepare-dataset": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-prepare-dataset"
          },
          "inputs": {
            "parameters": {
              "job_id": {
                "runtimeValue": {
                  "constant": "{{$.pipeline_job_uuid}}"
                }
              },
              "location": {
                "componentInputParameter": "location"
              },
              "project_id": {
                "componentInputParameter": "project_id"
              }
            }
          },
          "taskInfo": {
            "name": "prepare-dataset"
          }
        },
        "prepare-features": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-prepare-features"
          },
          "dependentTasks": [
            "clean-data",
            "prepare-dataset"
          ],
          "inputs": {
            "parameters": {
              "bq_dataset_id": {
                "taskOutputParameter": {
                  "outputParameterKey": "Output",
                  "producerTask": "prepare-dataset"
                }
              },
              "location": {
                "componentInputParameter": "location"
              },
              "project_id": {
                "componentInputParameter": "project_id"
              },
              "source_table_path": {
                "taskOutputParameter": {
                  "outputParameterKey": "Output",
                  "producerTask": "clean-data"
                }
              }
            }
          },
          "taskInfo": {
            "name": "prepare-features"
          }
        },
        "train-save-model": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-train-save-model"
          },
          "dependentTasks": [
            "prepare-features"
          ],
          "inputs": {
            "parameters": {
              "location": {
                "componentInputParameter": "location"
              },
              "model_name": {
                "runtimeValue": {
                  "constant": "taxi_fare_model"
                }
              },
              "project_id": {
                "componentInputParameter": "project_id"
              },
              "source_table_path": {
                "taskOutputParameter": {
                  "outputParameterKey": "Output",
                  "producerTask": "prepare-features"
                }
              },
              "staging_bucket": {
                "runtimeValue": {
                  "constant": "gs://vertexai-demo-pipeline"
                }
              }
            }
          },
          "taskInfo": {
            "name": "train-save-model"
          }
        }
      }
    },
    "inputDefinitions": {
      "parameters": {
        "location": {
          "parameterType": "STRING"
        },
        "project_id": {
          "parameterType": "STRING"
        },
        "source_table_path": {
          "parameterType": "STRING"
        }
      }
    }
  },
  "schemaVersion": "2.1.0",
  "sdkVersion": "kfp-2.8.0"
}